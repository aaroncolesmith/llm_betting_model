{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Soccer Betting Model - Testing Notebook\n",
                "\n",
                "This notebook is for testing the soccer betting model pipeline:\n",
                "1. Building prompts\n",
                "2. Evaluating bets\n",
                "3. Analyzing results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import datetime\n",
                "from pathlib import Path\n",
                "import sys\n",
                "\n",
                "# Add scripts directory to path\n",
                "sys.path.insert(0, '../scripts')\n",
                "\n",
                "# Import our custom functions\n",
                "from soccer_build_prompt import build_soccer_prompt, MODEL_LIST\n",
                "from soccer_evaluate_bets import evaluate_soccer_bets\n",
                "\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)\n",
                "pd.set_option('display.max_colwidth', 500)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Build Prompts for All Models\n",
                "\n",
                "This will:\n",
                "- Fetch upcoming soccer games\n",
                "- Load historical performance\n",
                "- Generate prompts for each model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build prompts for all models\n",
                "print(\"Building prompts for all soccer models...\\n\")\n",
                "\n",
                "for model_name in MODEL_LIST:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Building prompt for: {model_name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    try:\n",
                "        df_games = build_soccer_prompt(model_name)\n",
                "        print(f\"\\n✓ Successfully built prompt for {model_name}\")\n",
                "        print(f\"  Games included: {len(df_games)}\")\n",
                "    except Exception as e:\n",
                "        print(f\"\\n✗ Error building prompt for {model_name}: {e}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Prompt building complete!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Test Single Model\n",
                "\n",
                "Test with one model first to verify everything works"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with charlie model\n",
                "test_model = 'charlie'\n",
                "\n",
                "print(f\"Testing with model: {test_model}\\n\")\n",
                "df_test = build_soccer_prompt(test_model)\n",
                "\n",
                "print(f\"\\nGames DataFrame shape: {df_test.shape}\")\n",
                "print(f\"\\nFirst few games:\")\n",
                "df_test.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. View Generated Prompt\n",
                "\n",
                "Check the generated prompt file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read and display the generated prompt\n",
                "prompt_path = Path(f'../prompts/soccer_prompt_{test_model}.txt')\n",
                "\n",
                "if prompt_path.exists():\n",
                "    with open(prompt_path, 'r') as f:\n",
                "        prompt_content = f.read()\n",
                "    \n",
                "    print(f\"Prompt file: {prompt_path}\")\n",
                "    print(f\"Length: {len(prompt_content):,} characters\")\n",
                "    print(f\"\\nFirst 1000 characters:\")\n",
                "    print(\"-\" * 60)\n",
                "    print(prompt_content[:1000])\n",
                "    print(\"-\" * 60)\n",
                "else:\n",
                "    print(f\"Prompt file not found: {prompt_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Check Existing Evaluated Bets\n",
                "\n",
                "Load and analyze existing evaluated bets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load existing evaluated bets\n",
                "eval_path = Path('../data/soccer_bet_picks_evaluated.csv')\n",
                "\n",
                "if eval_path.exists():\n",
                "    df_eval = pd.read_csv(eval_path)\n",
                "    print(f\"Loaded {len(df_eval)} evaluated bets\")\n",
                "    print(f\"\\nModels: {df_eval['model'].unique().tolist()}\")\n",
                "    print(f\"Date range: {df_eval['date'].min()} to {df_eval['date'].max()}\")\n",
                "    \n",
                "    # Summary by model\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(\"Summary by Model\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    summary = df_eval.groupby('model').agg({\n",
                "        'bet_result': ['count', lambda x: (x == 'win').sum()],\n",
                "        'bet_payout': 'sum'\n",
                "    }).round(2)\n",
                "    \n",
                "    summary.columns = ['Total Bets', 'Wins', 'Total Payout']\n",
                "    summary['Win Rate %'] = (summary['Wins'] / summary['Total Bets'] * 100).round(1)\n",
                "    \n",
                "    print(summary)\n",
                "    \n",
                "    # Display recent bets\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(\"Recent Bets (Last 10)\")\n",
                "    print(f\"{'='*60}\")\n",
                "    df_eval.tail(10)\n",
                "else:\n",
                "    print(f\"No evaluated bets file found at: {eval_path}\")\n",
                "    print(\"This is normal if you haven't evaluated any bets yet.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluate Bets (if you have pending bets)\n",
                "\n",
                "This will evaluate any pending bets that have completed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if we have pending bets to evaluate\n",
                "pending_bets_path = Path(f'../data/soccer_bets_{test_model}.txt')\n",
                "\n",
                "if pending_bets_path.exists():\n",
                "    print(f\"Found pending bets file: {pending_bets_path}\")\n",
                "    \n",
                "    # Load and preview\n",
                "    df_pending = pd.read_csv(pending_bets_path)\n",
                "    print(f\"\\nPending bets: {len(df_pending)}\")\n",
                "    print(f\"\\nFirst few bets:\")\n",
                "    display(df_pending.head())\n",
                "    \n",
                "    # Evaluate\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Evaluating bets for {test_model}...\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    df_evaluated = evaluate_soccer_bets(test_model)\n",
                "    \n",
                "    if df_evaluated is not None:\n",
                "        print(f\"\\n✓ Evaluation complete!\")\n",
                "        display(df_evaluated.tail(10))\n",
                "else:\n",
                "    print(f\"No pending bets file found: {pending_bets_path}\")\n",
                "    print(\"You'll need to create this file with your model's picks first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Analyze Performance Over Time\n",
                "\n",
                "Visualize performance trends"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "if eval_path.exists():\n",
                "    df_eval = pd.read_csv(eval_path)\n",
                "    df_eval['date'] = pd.to_datetime(df_eval['date'])\n",
                "    \n",
                "    # Calculate cumulative payout by model\n",
                "    df_eval_sorted = df_eval.sort_values(['model', 'date'])\n",
                "    df_eval_sorted['cumulative_payout'] = df_eval_sorted.groupby('model')['bet_payout'].cumsum()\n",
                "    \n",
                "    # Plot\n",
                "    fig, ax = plt.subplots(figsize=(12, 6))\n",
                "    \n",
                "    for model in df_eval_sorted['model'].unique():\n",
                "        model_data = df_eval_sorted[df_eval_sorted['model'] == model]\n",
                "        ax.plot(model_data['date'], model_data['cumulative_payout'], \n",
                "                marker='o', label=model, linewidth=2)\n",
                "    \n",
                "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
                "    ax.set_xlabel('Date', fontsize=12)\n",
                "    ax.set_ylabel('Cumulative Payout (units)', fontsize=12)\n",
                "    ax.set_title('Soccer Betting Model Performance Over Time', fontsize=14, fontweight='bold')\n",
                "    ax.legend()\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No evaluation data available for plotting\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Quick Reference - File Locations\n",
                "\n",
                "Important file paths for the soccer betting pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Soccer Betting Model File Structure\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nPrompts:\")\n",
                "for model in MODEL_LIST:\n",
                "    path = Path(f'../prompts/soccer_prompt_{model}.txt')\n",
                "    status = \"✓\" if path.exists() else \"✗\"\n",
                "    print(f\"  {status} {path}\")\n",
                "\n",
                "print(\"\\nPending Bets:\")\n",
                "for model in MODEL_LIST:\n",
                "    path = Path(f'../data/soccer_bets_{model}.txt')\n",
                "    status = \"✓\" if path.exists() else \"✗\"\n",
                "    print(f\"  {status} {path}\")\n",
                "\n",
                "print(\"\\nEvaluated Bets:\")\n",
                "path = Path('../data/soccer_bet_picks_evaluated.csv')\n",
                "status = \"✓\" if path.exists() else \"✗\"\n",
                "print(f\"  {status} {path}\")\n",
                "\n",
                "print(\"\\nGame Results:\")\n",
                "path = Path('../data/soccer_game_results.csv')\n",
                "status = \"✓\" if path.exists() else \"✗\"\n",
                "print(f\"  {status} {path}\")\n",
                "\n",
                "print(\"\\nDatabase:\")\n",
                "path = Path('../data/soccer_bets_db.csv')\n",
                "status = \"✓\" if path.exists() else \"✗\"\n",
                "print(f\"  {status} {path}\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}